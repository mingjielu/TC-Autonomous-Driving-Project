WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/mmopenlab/mmcv/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
projects.mmdet3d_plugin
projects.mmdet3d_plugin
projects.mmdet3d_plugin
projects.mmdet3d_plugin
/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 08:32:11,620 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 08:32:12,212 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
/bin/sh: 1: /opt/rocm/hip/bin/hipcc: not found
fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 08:32:12,471 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

fatal: detected dubious ownership in repository at '/mnt/raid0/liuji/UniAD'
To add an exception for this directory, call:

	git config --global --add safe.directory /mnt/raid0/liuji/UniAD
2025-04-22 08:32:12,489 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
CUDA available: True
GPU 0,1,2,3,4,5: AMD Radeon Graphics
CUDA_HOME: /opt/rocm
NVCC: Not Available
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.1+gitcfc225a
PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - HIP Runtime 6.2.41134
  - MIOpen 3.2.0
  - Magma 2.7.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/cache/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, 

TorchVision: 0.14.0a0+befa256
OpenCV: 4.8.1
MMCV: 1.7.1
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 60241134
MMDetection: 2.26.0
MMSegmentation: 0.25.0
MMDetection3D: 1.0.0rc4+
spconv2.0: False
------------------------------------------------------------

2025-04-22 08:32:12,852 - mmdet - INFO - Distributed training: False
2025-04-22 08:32:13,895 - mmdet - INFO - Distributed training: False
2025-04-22 08:32:14,227 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 08:32:14,228 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 08:32:14,342 - mmdet - INFO - Distributed training: False
2025-04-22 08:32:14,352 - mmdet - INFO - Distributed training: False
2025-04-22 08:32:15,106 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 08:32:15,150 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 08:32:15,150 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 08:32:15,245 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,246 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,246 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,247 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,247 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,248 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,248 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,249 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,254 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,257 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,261 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,265 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,269 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,273 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,277 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,281 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,285 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,289 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,293 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,297 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,301 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,305 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,309 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,313 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,317 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,321 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,325 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,329 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,333 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,337 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,351 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,366 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,381 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:15,413 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,493 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,494 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,495 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,496 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,497 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,498 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,499 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,500 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,501 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,502 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,503 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,504 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,505 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,506 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,507 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,508 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,509 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,510 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,511 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,512 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,513 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,514 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,515 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,516 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,517 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,518 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,519 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,520 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:15,529 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
2025-04-22 08:32:15,904 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 08:32:15,904 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 08:32:16,028 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 08:32:16,163 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,164 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,164 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,165 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,165 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,166 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,166 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,168 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,172 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,176 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,179 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,183 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,187 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,191 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,195 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,199 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,203 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,206 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,210 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,214 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,218 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,222 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,226 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,230 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,234 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,238 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,241 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,245 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,249 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,253 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,265 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,280 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,289 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesE2EDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=True,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='ObjectRangeFilterTrack',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilterTrack',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img', 'timestamp',
            'l2g_r_mat', 'l2g_t', 'gt_fut_traj', 'gt_fut_traj_mask',
            'gt_past_traj', 'gt_past_traj_mask', 'gt_sdc_bbox', 'gt_sdc_label',
            'gt_sdc_fut_traj', 'gt_sdc_fut_traj_mask', 'gt_lane_labels',
            'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
            'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
            'gt_backward_flow', 'gt_occ_has_invalid_frame',
            'gt_occ_img_is_valid', 'gt_future_boxes', 'gt_future_labels',
            'sdc_planning', 'sdc_planning_mask', 'command'
        ])
]
test_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/nuscenes/'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnotations3D_E2E',
        with_bbox_3d=False,
        with_label_3d=False,
        with_attr_label=False,
        with_future_anns=True,
        with_ins_inds_3d=False,
        ins_inds_add_1=True),
    dict(
        type='GenerateOccFlowLabels',
        grid_conf=dict(
            xbound=[-50.0, 50.0, 0.5],
            ybound=[-50.0, 50.0, 0.5],
            zbound=[-10.0, 10.0, 20.0]),
        ignore_index=255,
        only_vehicle=True,
        filter_invisible=False),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_lane_labels',
                    'gt_lane_bboxes', 'gt_lane_masks', 'gt_segmentation',
                    'gt_instance', 'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,
    train=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_train.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=True,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='ObjectRangeFilterTrack',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilterTrack',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'gt_inds', 'img',
                    'timestamp', 'l2g_r_mat', 'l2g_t', 'gt_fut_traj',
                    'gt_fut_traj_mask', 'gt_past_traj', 'gt_past_traj_mask',
                    'gt_sdc_bbox', 'gt_sdc_label', 'gt_sdc_fut_traj',
                    'gt_sdc_fut_traj_mask', 'gt_lane_labels', 'gt_lane_bboxes',
                    'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                    'gt_centerness', 'gt_offset', 'gt_flow',
                    'gt_backward_flow', 'gt_occ_has_invalid_frame',
                    'gt_occ_img_is_valid', 'gt_future_boxes',
                    'gt_future_labels', 'sdc_planning', 'sdc_planning_mask',
                    'command'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        use_valid_flag=True,
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        queue_length=5,
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    val=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        use_nonlinear_optimizer=True,
        samples_per_gpu=1,
        eval_mod=['det', 'track', 'map'],
        occ_receptive_field=3,
        occ_n_future=6,
        occ_filter_invalid_sample=False),
    test=dict(
        type='NuScenesE2EDataset',
        data_root='data/nuscenes/',
        ann_file='data/infos/nuscenes_infos_temporal_val.pkl',
        pipeline=[
            dict(
                type='LoadMultiViewImageFromFilesInCeph',
                to_float32=True,
                file_client_args=dict(backend='disk'),
                img_root='data/nuscenes/'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnotations3D_E2E',
                with_bbox_3d=False,
                with_label_3d=False,
                with_attr_label=False,
                with_future_anns=True,
                with_ins_inds_3d=False,
                ins_inds_add_1=True),
            dict(
                type='GenerateOccFlowLabels',
                grid_conf=dict(
                    xbound=[-50.0, 50.0, 0.5],
                    ybound=[-50.0, 50.0, 0.5],
                    zbound=[-10.0, 10.0, 20.0]),
                ignore_index=255,
                only_vehicle=True,
                filter_invisible=False),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1600, 900),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                            'gt_lane_labels', 'gt_lane_bboxes',
                            'gt_lane_masks', 'gt_segmentation', 'gt_instance',
                            'gt_centerness', 'gt_offset', 'gt_flow',
                            'gt_backward_flow', 'gt_occ_has_invalid_frame',
                            'gt_occ_img_is_valid', 'sdc_planning',
                            'sdc_planning_mask', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        file_client_args=dict(backend='disk'),
        patch_size=[102.4, 102.4],
        canvas_size=(200, 200),
        bev_size=(200, 200),
        predict_steps=12,
        past_steps=4,
        fut_steps=4,
        occ_n_future=6,
        use_nonlinear_optimizer=True,
        eval_mod=['det', 'map', 'track']),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=6,
    pipeline=[
        dict(
            type='LoadMultiViewImageFromFilesInCeph',
            to_float32=True,
            file_client_args=dict(backend='disk'),
            img_root='data/nuscenes/'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[103.53, 116.28, 123.675],
            std=[1.0, 1.0, 1.0],
            to_rgb=False),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnotations3D_E2E',
            with_bbox_3d=False,
            with_label_3d=False,
            with_attr_label=False,
            with_future_anns=True,
            with_ins_inds_3d=False,
            ins_inds_add_1=True),
        dict(
            type='GenerateOccFlowLabels',
            grid_conf=dict(
                xbound=[-50.0, 50.0, 0.5],
                ybound=[-50.0, 50.0, 0.5],
                zbound=[-10.0, 10.0, 20.0]),
            ignore_index=255,
            only_vehicle=True,
            filter_invisible=False),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1600, 900),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'img', 'timestamp', 'l2g_r_mat', 'l2g_t',
                        'gt_lane_labels', 'gt_lane_bboxes', 'gt_lane_masks',
                        'gt_segmentation', 'gt_instance', 'gt_centerness',
                        'gt_offset', 'gt_flow', 'gt_backward_flow',
                        'gt_occ_has_invalid_frame', 'gt_occ_img_is_valid',
                        'sdc_planning', 'sdc_planning_mask', 'command'
                    ])
            ])
    ],
    planning_evaluation_strategy='uniad')
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './projects/work_dirs/stage1_track_map/base_track_map/'
load_from = 'ckpts/bevformer_r101_dcn_24ep.pth'
resume_from = None
workflow = [('train', 1)]
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
patch_size = [102.4, 102.4]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
_num_levels_ = 4
bev_h_ = 200
bev_w_ = 200
_feed_dim_ = 512
_dim_half_ = 128
canvas_size = (200, 200)
queue_length = 5
predict_steps = 12
predict_modes = 6
fut_steps = 4
past_steps = 4
use_nonlinear_optimizer = True
occ_n_future = 4
occ_n_future_plan = 6
occ_n_future_max = 6
planning_steps = 6
use_col_optim = True
planning_evaluation_strategy = 'uniad'
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
train_gt_iou_threshold = 0.3
model = dict(
    type='UniAD',
    gt_iou_threshold=0.3,
    queue_length=5,
    use_grid_mask=True,
    video_test_mode=True,
    num_query=900,
    num_classes=10,
    pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=4,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=256,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    freeze_img_backbone=True,
    freeze_img_neck=False,
    freeze_bn=False,
    score_thresh=0.4,
    filter_score_thresh=0.35,
    qim_args=dict(
        qim_type='QIMBase',
        merger_dropout=0,
        update_query_pos=True,
        fp_ratio=0.3,
        random_drop=0.1),
    mem_args=dict(
        memory_bank_type='MemoryBank',
        memory_bank_score_thresh=0.0,
        memory_bank_len=4),
    loss_cfg=dict(
        type='ClipMatcher',
        num_classes=10,
        weight_dict=None,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        assigner=dict(
            type='HungarianAssigner3DTrack',
            cls_cost=dict(type='FocalLossCost', weight=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_past_traj_weight=0.0),
    pts_bbox_head=dict(
        type='BEVFormerTrackHead',
        bev_h=200,
        bev_w=200,
        num_query=900,
        num_classes=10,
        in_channels=256,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        past_steps=4,
        fut_steps=4,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=256,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=256,
                            num_levels=1),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=256,
                                num_points=8,
                                num_levels=4),
                            embed_dims=256)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=256,
                            num_levels=1)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=128,
            row_num_embed=200,
            col_num_embed=200),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    seg_head=dict(
        type='PansegformerHead',
        bev_h=200,
        bev_w=200,
        canvas_size=(200, 200),
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        num_query=300,
        num_classes=4,
        num_things_classes=3,
        num_stuff_classes=1,
        in_channels=2048,
        sync_cls_avg_factor=True,
        as_two_stage=False,
        with_box_refine=True,
        transformer=dict(
            type='SegDeformableTransformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=4),
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DeformableDetrTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=256,
                            num_levels=4)
                    ],
                    feedforward_channels=512,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0),
        loss_mask=dict(type='DiceLoss', loss_weight=2.0),
        thing_transformer_head=dict(
            type='SegMaskHead', d_model=256, nhead=8, num_decoder_layers=4),
        stuff_transformer_head=dict(
            type='SegMaskHead',
            d_model=256,
            nhead=8,
            num_decoder_layers=6,
            self_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='HungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0)),
            assigner_with_mask=dict(
                type='HungarianAssigner_multi_info',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(
                    type='BBoxL1Cost', weight=5.0, box_format='xywh'),
                iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0),
                mask_cost=dict(type='DiceCost', weight=2.0)),
            sampler=dict(type='PseudoSampler'),
            sampler_with_mask=dict(type='PseudoSampler_segformer'))),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos/'
ann_file_train = 'data/infos/nuscenes_infos_temporal_train.pkl'
ann_file_val = 'data/infos/nuscenes_infos_temporal_val.pkl'
ann_file_test = 'data/infos/nuscenes_infos_temporal_val.pkl'
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(custom_keys=dict(img_backbone=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
total_epochs = 6
runner = dict(type='EpochBasedRunner', max_epochs=6)
find_unused_parameters = True
gpu_ids = range(0, 1)

2025-04-22 08:32:16,289 - mmdet - INFO - Set random seed to 0, deterministic: True
2025-04-22 08:32:16,293 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,324 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,404 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,405 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,406 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,407 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,408 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,409 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,410 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,411 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,412 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,413 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,414 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,415 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,416 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,417 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,418 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,419 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,420 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,421 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,422 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,423 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,424 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,425 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,426 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,427 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,428 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,429 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,430 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,431 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,432 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,432 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,432 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,432 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,432 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,432 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,432 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,432 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,432 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,432 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,432 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,432 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,432 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,432 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,432 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:16,441 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
2025-04-22 08:32:16,792 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 08:32:16,929 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,930 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,930 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,931 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,932 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,932 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,933 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,934 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,938 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,942 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,946 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,950 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,953 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,957 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,961 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,965 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,969 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,973 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,977 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,981 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,985 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,989 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,993 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:16,996 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,000 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,004 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,008 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,012 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,016 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,020 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,033 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,048 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,062 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,095 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 08:32:17,175 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,176 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,177 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,178 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,179 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,180 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,181 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,182 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,183 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,184 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,185 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,186 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,187 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,188 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,189 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,190 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,191 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,192 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,193 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,194 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,195 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,196 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,197 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,198 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,199 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,200 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,201 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,202 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,203 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,203 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,203 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,203 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,203 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,203 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,203 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,203 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,203 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,203 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,203 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,203 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,203 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,203 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,213 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
2025-04-22 08:32:17,323 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,324 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,324 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,325 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,325 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,326 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,327 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,328 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,332 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,336 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,340 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,345 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,349 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,353 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,357 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,361 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,365 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,369 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,373 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,378 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,382 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,386 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,390 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,394 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,398 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,403 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,407 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,411 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,415 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,419 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,435 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,451 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,465 - mmcv - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-22 08:32:17,503 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([200, 128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,585 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,586 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,587 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,588 - mmcv - INFO - 
pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,589 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,590 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([64, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.transformer.can_bus_mlp.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,591 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in BEVFormerTrackHead  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,592 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.0.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.1.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.2.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.3.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,593 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.4.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.weight - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
pts_bbox_head.past_traj_reg_branches.5.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
pts_bbox_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,594 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,595 - mmcv - INFO - 
img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,596 - mmcv - INFO - 
img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,597 - mmcv - INFO - 
img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,598 - mmcv - INFO - 
img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,599 - mmcv - INFO - 
img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,600 - mmcv - INFO - 
query_embedding.weight - torch.Size([901, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
reference_points.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.self_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.self_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.self_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.self_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.linear1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.linear1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.linear2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.linear2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.linear_pos1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.linear_pos1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.linear_pos2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.linear_pos2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.norm_pos.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.norm_pos.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.linear_feat1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.linear_feat1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.linear_feat2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.linear_feat2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.norm_feat.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.norm_feat.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
query_interact.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
memory_bank.save_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
memory_bank.save_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
memory_bank.temporal_attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
memory_bank.temporal_attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
memory_bank.temporal_fc1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
memory_bank.temporal_fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
memory_bank.temporal_fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
memory_bank.temporal_fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
memory_bank.temporal_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
memory_bank.temporal_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
memory_bank.temporal_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
memory_bank.temporal_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
seg_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,601 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,602 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,603 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,604 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,605 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.transformer.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.transformer.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.bev_embedding.weight - torch.Size([40000, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.cls_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.cls_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.cls_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.cls_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.cls_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.cls_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.cls_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.cls_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.cls_branches.4.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.cls_branches.4.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.cls_branches.5.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.cls_branches.5.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,606 - mmcv - INFO - 
seg_head.query_embedding.weight - torch.Size([300, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.stuff_query.weight - torch.Size([1, 512]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.reg_branches2.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_thing_branches.0.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_thing_branches.0.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_thing_branches.1.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_thing_branches.1.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_thing_branches.2.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_thing_branches.2.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_thing_branches.3.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_thing_branches.3.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_stuff_branches.0.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_stuff_branches.1.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_stuff_branches.2.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_stuff_branches.3.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_stuff_branches.4.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.cls_stuff_branches.5.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PansegformerHead  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,607 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,608 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.things_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.0.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,609 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.2.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,610 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.3.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.4.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.head_norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,611 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,612 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,612 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,612 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,612 - mmcv - INFO - 
seg_head.stuff_mask_head.blocks.5.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,612 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,612 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.q.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,612 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,612 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.k.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,612 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.weight - torch.Size([8, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,612 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear_l1.0.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,612 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.weight - torch.Size([1, 8]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,612 - mmcv - INFO - 
seg_head.stuff_mask_head.attnen.linear.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of UniAD  
 
2025-04-22 08:32:17,621 - mmdet - INFO - Model:
UniAD(
  (pts_bbox_head): BEVFormerTrackHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=200, col_num_embed=200)
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=64, bias=True)
                (attention_weights): Linear(in_features=256, out_features=32, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (past_traj_reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=16, bias=True)
      )
    )
    (bev_embedding): Embedding(40000, 256)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_embedding): Embedding(901, 512)
  (reference_points): Linear(in_features=256, out_features=3, bias=True)
  (query_interact): QueryInteractionModule(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0, inplace=False)
    (linear2): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos1): Linear(in_features=256, out_features=256, bias=True)
    (linear_pos2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_pos1): Dropout(p=0, inplace=False)
    (dropout_pos2): Dropout(p=0, inplace=False)
    (norm_pos): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (linear_feat1): Linear(in_features=256, out_features=256, bias=True)
    (linear_feat2): Linear(in_features=256, out_features=256, bias=True)
    (dropout_feat1): Dropout(p=0, inplace=False)
    (dropout_feat2): Dropout(p=0, inplace=False)
    (norm_feat): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0, inplace=False)
    (dropout2): Dropout(p=0, inplace=False)
  )
  (memory_bank): MemoryBank(
    (save_proj): Linear(in_features=256, out_features=256, bias=True)
    (temporal_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (temporal_fc1): Linear(in_features=256, out_features=256, bias=True)
    (temporal_fc2): Linear(in_features=256, out_features=256, bias=True)
    (temporal_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (temporal_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (criterion): ClipMatcher(
    (loss_cls): FocalLoss()
    (loss_bboxes): L1Loss()
    (loss_predictions): SmoothL1Loss()
  )
  (seg_head): PansegformerHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): SegDeformableTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DeformableDetrTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (bev_embedding): Embedding(40000, 256)
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
      (4): Linear(in_features=256, out_features=3, bias=True)
      (5): Linear(in_features=256, out_features=3, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embedding): Embedding(300, 512)
    (stuff_query): Embedding(1, 512)
    (reg_branches2): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (cls_thing_branches): ModuleList(
      (0): Linear(in_features=256, out_features=3, bias=True)
      (1): Linear(in_features=256, out_features=3, bias=True)
      (2): Linear(in_features=256, out_features=3, bias=True)
      (3): Linear(in_features=256, out_features=3, bias=True)
    )
    (cls_stuff_branches): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (loss_mask): DiceLoss()
    (things_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
    (stuff_mask_head): SegMaskHead(
      (blocks): ModuleList(
        (0): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (1): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (2): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (3): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (4): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
        (5): Block(
          (head_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=256, out_features=256, bias=True)
            (k): Linear(in_features=256, out_features=256, bias=True)
            (v): Linear(in_features=256, out_features=256, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (linear_l1): Sequential(
              (0): Linear(in_features=8, out_features=8, bias=True)
              (1): ReLU()
            )
            (linear): Sequential(
              (0): Linear(in_features=8, out_features=1, bias=True)
              (1): ReLU()
            )
          )
          (drop_path): Identity()
          (head_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0, inplace=False)
          )
          (self_attention): SelfAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
          )
          (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        )
      )
      (attnen): AttentionTail(
        (q): Linear(in_features=256, out_features=256, bias=True)
        (k): Linear(in_features=256, out_features=256, bias=True)
        (linear_l1): Sequential(
          (0): Linear(in_features=8, out_features=8, bias=True)
          (1): ReLU()
        )
        (linear): Sequential(
          (0): Linear(in_features=8, out_features=1, bias=True)
          (1): ReLU()
        )
      )
    )
  )
)
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.507 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.468 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.456 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 37.757 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.3 seconds.
======
Done reverse indexing in 6.4 seconds.
======
Done reverse indexing in 6.5 seconds.
======
Done reverse indexing in 6.6 seconds.
======
WARNING!!!!, Only can be used for obtain inference speed!!!!
======
Loading NuScenes tables for version v1.0-trainval...
WARNING!!!!, Only can be used for obtain inference speed!!!!
WARNING!!!!, Only can be used for obtain inference speed!!!!
======
Loading NuScenes tables for version v1.0-trainval...
WARNING!!!!, Only can be used for obtain inference speed!!!!
======
Loading NuScenes tables for version v1.0-trainval...
======
Loading NuScenes tables for version v1.0-trainval...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.521 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 34.897 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.099 seconds.
======
Reverse indexing ...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 35.110 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.3 seconds.
======
Done reverse indexing in 6.3 seconds.
======
Done reverse indexing in 6.5 seconds.
======
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 08:33:53,162 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 08:33:53,375 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 08:33:53,378 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 08:33:53,379 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 08:33:53,381 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 08:33:53,383 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 08:33:53,384 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 08:33:53,386 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 08:33:53,388 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 08:33:53,389 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 08:33:53,391 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 08:33:53,392 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 08:33:53,394 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 08:33:53,396 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 08:33:53,397 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 08:33:53,399 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 08:33:53,401 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 08:33:53,402 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 08:33:53,404 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 08:33:53,405 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 08:33:53,407 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 08:33:53,409 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 08:33:53,410 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 08:33:53,412 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 08:33:53,414 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 08:33:53,416 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 08:33:53,418 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 08:33:53,477 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 08:33:53,477 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 08:33:53,479 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 08:33:53,479 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 08:33:53,480 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 08:33:53,480 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 08:33:53,480 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 08:33:53,480 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 08:33:53,480 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 08:33:53,480 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 08:33:53,934 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 08:33:54,147 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 08:33:54,149 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 08:33:54,151 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 08:33:54,152 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 08:33:54,154 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 08:33:54,156 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 08:33:54,157 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 08:33:54,159 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 08:33:54,161 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 08:33:54,162 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 08:33:54,164 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 08:33:54,166 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 08:33:54,167 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 08:33:54,169 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 08:33:54,171 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 08:33:54,172 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 08:33:54,174 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 08:33:54,176 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 08:33:54,177 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 08:33:54,179 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 08:33:54,180 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 08:33:54,182 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 08:33:54,184 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 08:33:54,185 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 08:33:54,188 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 08:33:54,190 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 08:33:54,249 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 08:33:54,249 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 08:33:54,251 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 08:33:54,251 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 08:33:54,251 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 08:33:54,251 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 08:33:54,252 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 08:33:54,252 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 08:33:54,252 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 08:33:54,252 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 08:33:55,038 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 08:33:55,246 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 08:33:55,248 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 08:33:55,250 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 08:33:55,252 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 08:33:55,253 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 08:33:55,255 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 08:33:55,257 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 08:33:55,258 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 08:33:55,260 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 08:33:55,262 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 08:33:55,263 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 08:33:55,265 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 08:33:55,267 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 08:33:55,268 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 08:33:55,270 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 08:33:55,272 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 08:33:55,273 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 08:33:55,275 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 08:33:55,277 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 08:33:55,278 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 08:33:55,280 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 08:33:55,281 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 08:33:55,283 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 08:33:55,285 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 08:33:55,287 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 08:33:55,289 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 08:33:55,348 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 08:33:55,348 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 08:33:55,350 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 08:33:55,350 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 08:33:55,350 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 08:33:55,350 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 08:33:55,350 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 08:33:55,350 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 08:33:55,351 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 08:33:55,351 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 08:33:58,004 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:33:58,032 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:33:58,037 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:33:58,047 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:33:58,048 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:33:58,050 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:33:58,054 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:33:58,062 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:33:58,063 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:33:58,065 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:33:58,068 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:33:58,086 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:33:58,094 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:33:58,094 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:33:58,095 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:33:58,097 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:33:58,113 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:33:58,114 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:33:58,115 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:33:58,116 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:33:58,129 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:33:58,155 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:33:58,210 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:33:58,245 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:33:58,992 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:33:59,014 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:33:59,019 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:33:59,046 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:33:59,050 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:33:59,052 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:33:59,053 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:33:59,057 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:33:59,066 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:33:59,068 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:33:59,068 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:33:59,071 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:33:59,098 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:33:59,100 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:33:59,100 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:33:59,102 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:33:59,109 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:33:59,120 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:33:59,121 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:33:59,121 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:33:59,122 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:33:59,134 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:33:59,172 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:33:59,193 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:33:59,194 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:33:59,208 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:33:59,240 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:33:59,245 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:33:59,261 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:33:59,272 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:33:59,393 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:33:59,423 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:33:59,467 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:33:59,484 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:33:59,521 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:33:59,544 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:33:59,558 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:33:59,561 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:33:59,571 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:33:59,577 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:33:59,600 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:33:59,620 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:33:59,621 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:33:59,646 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:33:59,739 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:33:59,757 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:33:59,801 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:33:59,831 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:00,032 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:00,066 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:00,199 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:00,211 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:00,214 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:00,236 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:00,248 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:00,253 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:00,258 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:00,277 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:00,311 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:00,324 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:00,333 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:00,346 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:00,352 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:00,363 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:00,366 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:00,368 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:00,372 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:00,374 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:00,376 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:00,382 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:00,385 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:00,388 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:00,389 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:00,393 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:00,406 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:00,412 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:00,418 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:00,418 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:00,421 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:00,421 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:00,428 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:00,441 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:00,442 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:00,442 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:00,442 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:00,449 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:00,548 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:00,566 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:00,591 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:00,606 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:00,621 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:00,629 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:00,640 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:00,654 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:00,664 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:00,671 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:00,685 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:00,688 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:00,698 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:00,705 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:00,708 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:00,709 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:00,721 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:00,721 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:00,728 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:00,733 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:00,744 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:00,749 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:00,754 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:00,755 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:00,772 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:00,773 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:01,056 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:01,093 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:01,182 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:01,205 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:01,207 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:01,241 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:01,253 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:01,282 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:01,290 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:01,311 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:01,352 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:01,356 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:01,373 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:01,377 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:01,388 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:01,413 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:01,414 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:01,440 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:01,446 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:01,464 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:01,502 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:01,524 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:01,691 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:01,711 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:01,748 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:01,773 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:01,837 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:01,871 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:01,873 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:01,910 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:01,947 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:01,964 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:01,995 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:02,016 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:02,021 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:02,022 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:02,039 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:02,039 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:02,069 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:02,077 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:02,077 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:02,087 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:02,096 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:02,100 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:02,100 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:02,111 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:02,115 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:02,128 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:02,128 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:02,139 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:02,150 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:02,150 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:02,163 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:02,163 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:02,169 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:02,174 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:02,181 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:02,185 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:02,192 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:02,196 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:02,209 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:02,225 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:02,229 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:02,254 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:02,260 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:02,266 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:02,283 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:02,294 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:02,297 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:02,310 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:02,335 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:02,346 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:02,351 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:02,369 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:02,388 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:02,415 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:02,448 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:02,481 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:02,524 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:02,541 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:02,579 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:02,605 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:02,632 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:02,643 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:02,653 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:02,667 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:02,698 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:02,710 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:02,730 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:02,740 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:02,892 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:02,910 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:02,928 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:02,957 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:02,965 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:02,981 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:02,999 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:03,015 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:03,043 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:03,055 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:03,069 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:03,082 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:03,132 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:03,146 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:03,177 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:03,196 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:03,204 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:03,238 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:03,445 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:03,481 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:03,485 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:03,504 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:03,538 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:03,546 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:03,559 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:03,568 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:03,575 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:03,589 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:03,600 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:03,624 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:03,633 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:03,658 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:03,677 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:03,684 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:03,695 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:03,701 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:03,732 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:03,739 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:03,751 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:03,757 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:03,760 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:03,765 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:03,769 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:03,792 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:03,797 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:03,810 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:03,815 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:03,833 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:03,836 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:03,849 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:03,852 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:03,871 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:03,886 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:03,910 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:03,928 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:03,947 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:03,949 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:03,951 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:03,960 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:03,964 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:03,988 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:04,002 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:04,007 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:04,029 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:04,034 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:04,052 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:04,084 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:04,110 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:04,136 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:04,158 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:04,198 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:04,228 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:04,427 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:04,454 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:04,497 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:04,507 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:04,528 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:04,536 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:04,603 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:04,647 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:04,663 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:04,686 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:04,691 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:04,717 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:04,731 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:04,744 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:04,752 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:04,772 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:04,790 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:04,793 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:04,921 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:04,923 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:04,938 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:04,939 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:04,940 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:04,954 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:04,963 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:04,971 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:04,974 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:04,975 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:04,998 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:04,999 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:05,007 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:05,008 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:05,025 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:05,031 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:05,033 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:05,060 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:05,066 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
Done reverse indexing in 21.5 seconds.
======
2025-04-22 08:34:05,083 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:05,126 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:05,150 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:05,152 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:05,167 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:05,172 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:05,190 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:05,205 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:05,229 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:05,229 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:05,230 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:05,249 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:05,255 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:05,289 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:05,315 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:05,356 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:05,374 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:05,414 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:05,440 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:05,451 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:05,472 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:05,512 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:05,541 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:05,541 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:05,548 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:05,558 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:05,567 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:05,599 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:05,609 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:05,625 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:05,634 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:05,704 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:05,723 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:05,766 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:05,792 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:05,799 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:05,817 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:05,829 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:05,842 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:05,855 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:05,872 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:05,881 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:05,892 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:05,912 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:05,928 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:05,941 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:05,957 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:05,969 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:05,975 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:05,984 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:06,001 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:06,014 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:06,028 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:06,036 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:06,039 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:06,051 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:06,060 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:06,252 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:06,269 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:06,294 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:06,311 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:06,313 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:06,339 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:06,353 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:06,371 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:06,379 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:06,389 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:06,425 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:06,429 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:06,443 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:06,455 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:06,460 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:06,479 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:06,487 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:06,510 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:06,514 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:06,520 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:06,530 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:06,549 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:06,571 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:06,597 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:06,645 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:06,664 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:06,703 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:06,727 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:06,736 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:06,760 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:06,798 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:06,826 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:06,983 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:06,996 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,027 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,047 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:07,069 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:07,087 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,087 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:07,107 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:07,125 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,127 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,127 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,136 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:07,154 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:07,155 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,169 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,190 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,192 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,193 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:07,216 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:07,217 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:07,224 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:07,233 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,250 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:07,267 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,270 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,293 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:07,304 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,327 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:07,369 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:07,386 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,413 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:07,419 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:07,426 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,447 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,453 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,453 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:07,454 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:07,477 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,501 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,502 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,511 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:07,523 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:07,526 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,530 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,536 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:07,538 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:07,547 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,553 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:07,558 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,562 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:07,568 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,572 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,580 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:07,589 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:07,596 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:07,605 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,613 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,628 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:07,650 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,675 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:07,731 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:07,749 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:07,792 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:07,818 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
WARNING!!!!, Only can be used for obtain inference speed!!!!
2025-04-22 08:34:07,849 - mmdet - INFO - load checkpoint from local path: ckpts/bevformer_r101_dcn_24ep.pth
2025-04-22 08:34:07,999 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,020 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,020 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,040 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,058 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,065 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,077 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,082 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,086 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.
2025-04-22 08:34:08,089 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.
2025-04-22 08:34:08,091 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.
2025-04-22 08:34:08,092 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.
2025-04-22 08:34:08,095 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,097 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.
2025-04-22 08:34:08,099 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.
2025-04-22 08:34:08,101 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,101 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.
2025-04-22 08:34:08,103 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.
2025-04-22 08:34:08,105 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.
2025-04-22 08:34:08,107 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.
2025-04-22 08:34:08,108 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.
2025-04-22 08:34:08,110 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.
2025-04-22 08:34:08,112 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,112 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.
2025-04-22 08:34:08,114 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.
2025-04-22 08:34:08,115 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,116 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.
2025-04-22 08:34:08,117 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.
2025-04-22 08:34:08,119 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.
2025-04-22 08:34:08,121 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.
2025-04-22 08:34:08,122 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,123 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.
2025-04-22 08:34:08,123 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,125 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.
2025-04-22 08:34:08,126 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.
2025-04-22 08:34:08,128 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.
2025-04-22 08:34:08,130 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.
2025-04-22 08:34:08,132 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.
2025-04-22 08:34:08,135 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.
2025-04-22 08:34:08,137 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.
2025-04-22 08:34:08,148 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,150 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,151 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,164 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,170 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,181 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,194 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,198 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 08:34:08,198 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.query_embedding.weight, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias

missing keys in source state_dict: pts_bbox_head.past_traj_reg_branches.0.0.weight, pts_bbox_head.past_traj_reg_branches.0.0.bias, pts_bbox_head.past_traj_reg_branches.0.2.weight, pts_bbox_head.past_traj_reg_branches.0.2.bias, pts_bbox_head.past_traj_reg_branches.0.4.weight, pts_bbox_head.past_traj_reg_branches.0.4.bias, pts_bbox_head.past_traj_reg_branches.1.0.weight, pts_bbox_head.past_traj_reg_branches.1.0.bias, pts_bbox_head.past_traj_reg_branches.1.2.weight, pts_bbox_head.past_traj_reg_branches.1.2.bias, pts_bbox_head.past_traj_reg_branches.1.4.weight, pts_bbox_head.past_traj_reg_branches.1.4.bias, pts_bbox_head.past_traj_reg_branches.2.0.weight, pts_bbox_head.past_traj_reg_branches.2.0.bias, pts_bbox_head.past_traj_reg_branches.2.2.weight, pts_bbox_head.past_traj_reg_branches.2.2.bias, pts_bbox_head.past_traj_reg_branches.2.4.weight, pts_bbox_head.past_traj_reg_branches.2.4.bias, pts_bbox_head.past_traj_reg_branches.3.0.weight, pts_bbox_head.past_traj_reg_branches.3.0.bias, pts_bbox_head.past_traj_reg_branches.3.2.weight, pts_bbox_head.past_traj_reg_branches.3.2.bias, pts_bbox_head.past_traj_reg_branches.3.4.weight, pts_bbox_head.past_traj_reg_branches.3.4.bias, pts_bbox_head.past_traj_reg_branches.4.0.weight, pts_bbox_head.past_traj_reg_branches.4.0.bias, pts_bbox_head.past_traj_reg_branches.4.2.weight, pts_bbox_head.past_traj_reg_branches.4.2.bias, pts_bbox_head.past_traj_reg_branches.4.4.weight, pts_bbox_head.past_traj_reg_branches.4.4.bias, pts_bbox_head.past_traj_reg_branches.5.0.weight, pts_bbox_head.past_traj_reg_branches.5.0.bias, pts_bbox_head.past_traj_reg_branches.5.2.weight, pts_bbox_head.past_traj_reg_branches.5.2.bias, pts_bbox_head.past_traj_reg_branches.5.4.weight, pts_bbox_head.past_traj_reg_branches.5.4.bias, query_embedding.weight, reference_points.weight, reference_points.bias, query_interact.self_attn.in_proj_weight, query_interact.self_attn.in_proj_bias, query_interact.self_attn.out_proj.weight, query_interact.self_attn.out_proj.bias, query_interact.linear1.weight, query_interact.linear1.bias, query_interact.linear2.weight, query_interact.linear2.bias, query_interact.linear_pos1.weight, query_interact.linear_pos1.bias, query_interact.linear_pos2.weight, query_interact.linear_pos2.bias, query_interact.norm_pos.weight, query_interact.norm_pos.bias, query_interact.linear_feat1.weight, query_interact.linear_feat1.bias, query_interact.linear_feat2.weight, query_interact.linear_feat2.bias, query_interact.norm_feat.weight, query_interact.norm_feat.bias, query_interact.norm1.weight, query_interact.norm1.bias, query_interact.norm2.weight, query_interact.norm2.bias, memory_bank.save_proj.weight, memory_bank.save_proj.bias, memory_bank.temporal_attn.in_proj_weight, memory_bank.temporal_attn.in_proj_bias, memory_bank.temporal_attn.out_proj.weight, memory_bank.temporal_attn.out_proj.bias, memory_bank.temporal_fc1.weight, memory_bank.temporal_fc1.bias, memory_bank.temporal_fc2.weight, memory_bank.temporal_fc2.bias, memory_bank.temporal_norm1.weight, memory_bank.temporal_norm1.bias, memory_bank.temporal_norm2.weight, memory_bank.temporal_norm2.bias, criterion.code_weights, seg_head.transformer.level_embeds, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.0.norms.0.weight, seg_head.transformer.encoder.layers.0.norms.0.bias, seg_head.transformer.encoder.layers.0.norms.1.weight, seg_head.transformer.encoder.layers.0.norms.1.bias, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.1.norms.0.weight, seg_head.transformer.encoder.layers.1.norms.0.bias, seg_head.transformer.encoder.layers.1.norms.1.weight, seg_head.transformer.encoder.layers.1.norms.1.bias, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.2.norms.0.weight, seg_head.transformer.encoder.layers.2.norms.0.bias, seg_head.transformer.encoder.layers.2.norms.1.weight, seg_head.transformer.encoder.layers.2.norms.1.bias, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.3.norms.0.weight, seg_head.transformer.encoder.layers.3.norms.0.bias, seg_head.transformer.encoder.layers.3.norms.1.weight, seg_head.transformer.encoder.layers.3.norms.1.bias, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.4.norms.0.weight, seg_head.transformer.encoder.layers.4.norms.0.bias, seg_head.transformer.encoder.layers.4.norms.1.weight, seg_head.transformer.encoder.layers.4.norms.1.bias, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, seg_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, seg_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, seg_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.encoder.layers.5.norms.0.weight, seg_head.transformer.encoder.layers.5.norms.0.bias, seg_head.transformer.encoder.layers.5.norms.1.weight, seg_head.transformer.encoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.0.norms.0.weight, seg_head.transformer.decoder.layers.0.norms.0.bias, seg_head.transformer.decoder.layers.0.norms.1.weight, seg_head.transformer.decoder.layers.0.norms.1.bias, seg_head.transformer.decoder.layers.0.norms.2.weight, seg_head.transformer.decoder.layers.0.norms.2.bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.1.norms.0.weight, seg_head.transformer.decoder.layers.1.norms.0.bias, seg_head.transformer.decoder.layers.1.norms.1.weight, seg_head.transformer.decoder.layers.1.norms.1.bias, seg_head.transformer.decoder.layers.1.norms.2.weight, seg_head.transformer.decoder.layers.1.norms.2.bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.2.norms.0.weight, seg_head.transformer.decoder.layers.2.norms.0.bias, seg_head.transformer.decoder.layers.2.norms.1.weight, seg_head.transformer.decoder.layers.2.norms.1.bias, seg_head.transformer.decoder.layers.2.norms.2.weight, seg_head.transformer.decoder.layers.2.norms.2.bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.3.norms.0.weight, seg_head.transformer.decoder.layers.3.norms.0.bias, seg_head.transformer.decoder.layers.3.norms.1.weight, seg_head.transformer.decoder.layers.3.norms.1.bias, seg_head.transformer.decoder.layers.3.norms.2.weight, seg_head.transformer.decoder.layers.3.norms.2.bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.4.norms.0.weight, seg_head.transformer.decoder.layers.4.norms.0.bias, seg_head.transformer.decoder.layers.4.norms.1.weight, seg_head.transformer.decoder.layers.4.norms.1.bias, seg_head.transformer.decoder.layers.4.norms.2.weight, seg_head.transformer.decoder.layers.4.norms.2.bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, seg_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, seg_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, seg_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, seg_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, seg_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, seg_head.transformer.decoder.layers.5.norms.0.weight, seg_head.transformer.decoder.layers.5.norms.0.bias, seg_head.transformer.decoder.layers.5.norms.1.weight, seg_head.transformer.decoder.layers.5.norms.1.bias, seg_head.transformer.decoder.layers.5.norms.2.weight, seg_head.transformer.decoder.layers.5.norms.2.bias, seg_head.transformer.reference_points.weight, seg_head.transformer.reference_points.bias, seg_head.bev_embedding.weight, seg_head.cls_branches.0.weight, seg_head.cls_branches.0.bias, seg_head.cls_branches.1.weight, seg_head.cls_branches.1.bias, seg_head.cls_branches.2.weight, seg_head.cls_branches.2.bias, seg_head.cls_branches.3.weight, seg_head.cls_branches.3.bias, seg_head.cls_branches.4.weight, seg_head.cls_branches.4.bias, seg_head.cls_branches.5.weight, seg_head.cls_branches.5.bias, seg_head.reg_branches.0.0.weight, seg_head.reg_branches.0.0.bias, seg_head.reg_branches.0.2.weight, seg_head.reg_branches.0.2.bias, seg_head.reg_branches.0.4.weight, seg_head.reg_branches.0.4.bias, seg_head.reg_branches.1.0.weight, seg_head.reg_branches.1.0.bias, seg_head.reg_branches.1.2.weight, seg_head.reg_branches.1.2.bias, seg_head.reg_branches.1.4.weight, seg_head.reg_branches.1.4.bias, seg_head.reg_branches.2.0.weight, seg_head.reg_branches.2.0.bias, seg_head.reg_branches.2.2.weight, seg_head.reg_branches.2.2.bias, seg_head.reg_branches.2.4.weight, seg_head.reg_branches.2.4.bias, seg_head.reg_branches.3.0.weight, seg_head.reg_branches.3.0.bias, seg_head.reg_branches.3.2.weight, seg_head.reg_branches.3.2.bias, seg_head.reg_branches.3.4.weight, seg_head.reg_branches.3.4.bias, seg_head.reg_branches.4.0.weight, seg_head.reg_branches.4.0.bias, seg_head.reg_branches.4.2.weight, seg_head.reg_branches.4.2.bias, seg_head.reg_branches.4.4.weight, seg_head.reg_branches.4.4.bias, seg_head.reg_branches.5.0.weight, seg_head.reg_branches.5.0.bias, seg_head.reg_branches.5.2.weight, seg_head.reg_branches.5.2.bias, seg_head.reg_branches.5.4.weight, seg_head.reg_branches.5.4.bias, seg_head.query_embedding.weight, seg_head.stuff_query.weight, seg_head.reg_branches2.0.0.weight, seg_head.reg_branches2.0.0.bias, seg_head.reg_branches2.0.2.weight, seg_head.reg_branches2.0.2.bias, seg_head.reg_branches2.0.4.weight, seg_head.reg_branches2.0.4.bias, seg_head.reg_branches2.1.0.weight, seg_head.reg_branches2.1.0.bias, seg_head.reg_branches2.1.2.weight, seg_head.reg_branches2.1.2.bias, seg_head.reg_branches2.1.4.weight, seg_head.reg_branches2.1.4.bias, seg_head.reg_branches2.2.0.weight, seg_head.reg_branches2.2.0.bias, seg_head.reg_branches2.2.2.weight, seg_head.reg_branches2.2.2.bias, seg_head.reg_branches2.2.4.weight, seg_head.reg_branches2.2.4.bias, seg_head.reg_branches2.3.0.weight, seg_head.reg_branches2.3.0.bias, seg_head.reg_branches2.3.2.weight, seg_head.reg_branches2.3.2.bias, seg_head.reg_branches2.3.4.weight, seg_head.reg_branches2.3.4.bias, seg_head.cls_thing_branches.0.weight, seg_head.cls_thing_branches.0.bias, seg_head.cls_thing_branches.1.weight, seg_head.cls_thing_branches.1.bias, seg_head.cls_thing_branches.2.weight, seg_head.cls_thing_branches.2.bias, seg_head.cls_thing_branches.3.weight, seg_head.cls_thing_branches.3.bias, seg_head.cls_stuff_branches.0.weight, seg_head.cls_stuff_branches.0.bias, seg_head.cls_stuff_branches.1.weight, seg_head.cls_stuff_branches.1.bias, seg_head.cls_stuff_branches.2.weight, seg_head.cls_stuff_branches.2.bias, seg_head.cls_stuff_branches.3.weight, seg_head.cls_stuff_branches.3.bias, seg_head.cls_stuff_branches.4.weight, seg_head.cls_stuff_branches.4.bias, seg_head.cls_stuff_branches.5.weight, seg_head.cls_stuff_branches.5.bias, seg_head.things_mask_head.blocks.0.head_norm1.weight, seg_head.things_mask_head.blocks.0.head_norm1.bias, seg_head.things_mask_head.blocks.0.attn.q.weight, seg_head.things_mask_head.blocks.0.attn.q.bias, seg_head.things_mask_head.blocks.0.attn.k.weight, seg_head.things_mask_head.blocks.0.attn.k.bias, seg_head.things_mask_head.blocks.0.attn.v.weight, seg_head.things_mask_head.blocks.0.attn.v.bias, seg_head.things_mask_head.blocks.0.attn.proj.weight, seg_head.things_mask_head.blocks.0.attn.proj.bias, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.0.attn.linear.0.weight, seg_head.things_mask_head.blocks.0.attn.linear.0.bias, seg_head.things_mask_head.blocks.0.head_norm2.weight, seg_head.things_mask_head.blocks.0.head_norm2.bias, seg_head.things_mask_head.blocks.0.mlp.fc1.weight, seg_head.things_mask_head.blocks.0.mlp.fc1.bias, seg_head.things_mask_head.blocks.0.mlp.fc2.weight, seg_head.things_mask_head.blocks.0.mlp.fc2.bias, seg_head.things_mask_head.blocks.1.head_norm1.weight, seg_head.things_mask_head.blocks.1.head_norm1.bias, seg_head.things_mask_head.blocks.1.attn.q.weight, seg_head.things_mask_head.blocks.1.attn.q.bias, seg_head.things_mask_head.blocks.1.attn.k.weight, seg_head.things_mask_head.blocks.1.attn.k.bias, seg_head.things_mask_head.blocks.1.attn.v.weight, seg_head.things_mask_head.blocks.1.attn.v.bias, seg_head.things_mask_head.blocks.1.attn.proj.weight, seg_head.things_mask_head.blocks.1.attn.proj.bias, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.1.attn.linear.0.weight, seg_head.things_mask_head.blocks.1.attn.linear.0.bias, seg_head.things_mask_head.blocks.1.head_norm2.weight, seg_head.things_mask_head.blocks.1.head_norm2.bias, seg_head.things_mask_head.blocks.1.mlp.fc1.weight, seg_head.things_mask_head.blocks.1.mlp.fc1.bias, seg_head.things_mask_head.blocks.1.mlp.fc2.weight, seg_head.things_mask_head.blocks.1.mlp.fc2.bias, seg_head.things_mask_head.blocks.2.head_norm1.weight, seg_head.things_mask_head.blocks.2.head_norm1.bias, seg_head.things_mask_head.blocks.2.attn.q.weight, seg_head.things_mask_head.blocks.2.attn.q.bias, seg_head.things_mask_head.blocks.2.attn.k.weight, seg_head.things_mask_head.blocks.2.attn.k.bias, seg_head.things_mask_head.blocks.2.attn.v.weight, seg_head.things_mask_head.blocks.2.attn.v.bias, seg_head.things_mask_head.blocks.2.attn.proj.weight, seg_head.things_mask_head.blocks.2.attn.proj.bias, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.2.attn.linear.0.weight, seg_head.things_mask_head.blocks.2.attn.linear.0.bias, seg_head.things_mask_head.blocks.2.head_norm2.weight, seg_head.things_mask_head.blocks.2.head_norm2.bias, seg_head.things_mask_head.blocks.2.mlp.fc1.weight, seg_head.things_mask_head.blocks.2.mlp.fc1.bias, seg_head.things_mask_head.blocks.2.mlp.fc2.weight, seg_head.things_mask_head.blocks.2.mlp.fc2.bias, seg_head.things_mask_head.blocks.3.head_norm1.weight, seg_head.things_mask_head.blocks.3.head_norm1.bias, seg_head.things_mask_head.blocks.3.attn.q.weight, seg_head.things_mask_head.blocks.3.attn.q.bias, seg_head.things_mask_head.blocks.3.attn.k.weight, seg_head.things_mask_head.blocks.3.attn.k.bias, seg_head.things_mask_head.blocks.3.attn.v.weight, seg_head.things_mask_head.blocks.3.attn.v.bias, seg_head.things_mask_head.blocks.3.attn.proj.weight, seg_head.things_mask_head.blocks.3.attn.proj.bias, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.things_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.things_mask_head.blocks.3.attn.linear.0.weight, seg_head.things_mask_head.blocks.3.attn.linear.0.bias, seg_head.things_mask_head.blocks.3.head_norm2.weight, seg_head.things_mask_head.blocks.3.head_norm2.bias, seg_head.things_mask_head.blocks.3.mlp.fc1.weight, seg_head.things_mask_head.blocks.3.mlp.fc1.bias, seg_head.things_mask_head.blocks.3.mlp.fc2.weight, seg_head.things_mask_head.blocks.3.mlp.fc2.bias, seg_head.things_mask_head.attnen.q.weight, seg_head.things_mask_head.attnen.q.bias, seg_head.things_mask_head.attnen.k.weight, seg_head.things_mask_head.attnen.k.bias, seg_head.things_mask_head.attnen.linear_l1.0.weight, seg_head.things_mask_head.attnen.linear_l1.0.bias, seg_head.things_mask_head.attnen.linear.0.weight, seg_head.things_mask_head.attnen.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm1.weight, seg_head.stuff_mask_head.blocks.0.head_norm1.bias, seg_head.stuff_mask_head.blocks.0.attn.q.weight, seg_head.stuff_mask_head.blocks.0.attn.q.bias, seg_head.stuff_mask_head.blocks.0.attn.k.weight, seg_head.stuff_mask_head.blocks.0.attn.k.bias, seg_head.stuff_mask_head.blocks.0.attn.v.weight, seg_head.stuff_mask_head.blocks.0.attn.v.bias, seg_head.stuff_mask_head.blocks.0.attn.proj.weight, seg_head.stuff_mask_head.blocks.0.attn.proj.bias, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.0.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.0.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.0.head_norm2.weight, seg_head.stuff_mask_head.blocks.0.head_norm2.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.0.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.0.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.0.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.0.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.0.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.0.norm3.weight, seg_head.stuff_mask_head.blocks.0.norm3.bias, seg_head.stuff_mask_head.blocks.1.head_norm1.weight, seg_head.stuff_mask_head.blocks.1.head_norm1.bias, seg_head.stuff_mask_head.blocks.1.attn.q.weight, seg_head.stuff_mask_head.blocks.1.attn.q.bias, seg_head.stuff_mask_head.blocks.1.attn.k.weight, seg_head.stuff_mask_head.blocks.1.attn.k.bias, seg_head.stuff_mask_head.blocks.1.attn.v.weight, seg_head.stuff_mask_head.blocks.1.attn.v.bias, seg_head.stuff_mask_head.blocks.1.attn.proj.weight, seg_head.stuff_mask_head.blocks.1.attn.proj.bias, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.1.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.1.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.1.head_norm2.weight, seg_head.stuff_mask_head.blocks.1.head_norm2.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.1.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.1.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.1.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.1.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.1.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.1.norm3.weight, seg_head.stuff_mask_head.blocks.1.norm3.bias, seg_head.stuff_mask_head.blocks.2.head_norm1.weight, seg_head.stuff_mask_head.blocks.2.head_norm1.bias, seg_head.stuff_mask_head.blocks.2.attn.q.weight, seg_head.stuff_mask_head.blocks.2.attn.q.bias, seg_head.stuff_mask_head.blocks.2.attn.k.weight, seg_head.stuff_mask_head.blocks.2.attn.k.bias, seg_head.stuff_mask_head.blocks.2.attn.v.weight, seg_head.stuff_mask_head.blocks.2.attn.v.bias, seg_head.stuff_mask_head.blocks.2.attn.proj.weight, seg_head.stuff_mask_head.blocks.2.attn.proj.bias, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.2.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.2.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.2.head_norm2.weight, seg_head.stuff_mask_head.blocks.2.head_norm2.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.2.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.2.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.2.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.2.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.2.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.2.norm3.weight, seg_head.stuff_mask_head.blocks.2.norm3.bias, seg_head.stuff_mask_head.blocks.3.head_norm1.weight, seg_head.stuff_mask_head.blocks.3.head_norm1.bias, seg_head.stuff_mask_head.blocks.3.attn.q.weight, seg_head.stuff_mask_head.blocks.3.attn.q.bias, seg_head.stuff_mask_head.blocks.3.attn.k.weight, seg_head.stuff_mask_head.blocks.3.attn.k.bias, seg_head.stuff_mask_head.blocks.3.attn.v.weight, seg_head.stuff_mask_head.blocks.3.attn.v.bias, seg_head.stuff_mask_head.blocks.3.attn.proj.weight, seg_head.stuff_mask_head.blocks.3.attn.proj.bias, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.3.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.3.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.3.head_norm2.weight, seg_head.stuff_mask_head.blocks.3.head_norm2.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.3.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.3.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.3.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.3.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.3.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.3.norm3.weight, seg_head.stuff_mask_head.blocks.3.norm3.bias, seg_head.stuff_mask_head.blocks.4.head_norm1.weight, seg_head.stuff_mask_head.blocks.4.head_norm1.bias, seg_head.stuff_mask_head.blocks.4.attn.q.weight, seg_head.stuff_mask_head.blocks.4.attn.q.bias, seg_head.stuff_mask_head.blocks.4.attn.k.weight, seg_head.stuff_mask_head.blocks.4.attn.k.bias, seg_head.stuff_mask_head.blocks.4.attn.v.weight, seg_head.stuff_mask_head.blocks.4.attn.v.bias, seg_head.stuff_mask_head.blocks.4.attn.proj.weight, seg_head.stuff_mask_head.blocks.4.attn.proj.bias, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.4.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.4.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.4.head_norm2.weight, seg_head.stuff_mask_head.blocks.4.head_norm2.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.4.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.4.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.4.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.4.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.4.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.4.norm3.weight, seg_head.stuff_mask_head.blocks.4.norm3.bias, seg_head.stuff_mask_head.blocks.5.head_norm1.weight, seg_head.stuff_mask_head.blocks.5.head_norm1.bias, seg_head.stuff_mask_head.blocks.5.attn.q.weight, seg_head.stuff_mask_head.blocks.5.attn.q.bias, seg_head.stuff_mask_head.blocks.5.attn.k.weight, seg_head.stuff_mask_head.blocks.5.attn.k.bias, seg_head.stuff_mask_head.blocks.5.attn.v.weight, seg_head.stuff_mask_head.blocks.5.attn.v.bias, seg_head.stuff_mask_head.blocks.5.attn.proj.weight, seg_head.stuff_mask_head.blocks.5.attn.proj.bias, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear_l1.0.bias, seg_head.stuff_mask_head.blocks.5.attn.linear.0.weight, seg_head.stuff_mask_head.blocks.5.attn.linear.0.bias, seg_head.stuff_mask_head.blocks.5.head_norm2.weight, seg_head.stuff_mask_head.blocks.5.head_norm2.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc1.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc1.bias, seg_head.stuff_mask_head.blocks.5.mlp.fc2.weight, seg_head.stuff_mask_head.blocks.5.mlp.fc2.bias, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.weight, seg_head.stuff_mask_head.blocks.5.self_attention.qkv.bias, seg_head.stuff_mask_head.blocks.5.self_attention.proj.weight, seg_head.stuff_mask_head.blocks.5.self_attention.proj.bias, seg_head.stuff_mask_head.blocks.5.norm3.weight, seg_head.stuff_mask_head.blocks.5.norm3.bias, seg_head.stuff_mask_head.attnen.q.weight, seg_head.stuff_mask_head.attnen.q.bias, seg_head.stuff_mask_head.attnen.k.weight, seg_head.stuff_mask_head.attnen.k.bias, seg_head.stuff_mask_head.attnen.linear_l1.0.weight, seg_head.stuff_mask_head.attnen.linear_l1.0.bias, seg_head.stuff_mask_head.attnen.linear.0.weight, seg_head.stuff_mask_head.attnen.linear.0.bias

2025-04-22 08:34:08,200 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 08:34:08,200 - mmdet - INFO - Start running, host: liuji@hjbog-srdc-20.amd.com, work_dir: /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map
2025-04-22 08:34:08,201 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 08:34:08,201 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-22 08:34:08,203 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 08:34:08,203 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-22 08:34:08,204 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 08:34:08,204 - mmdet - INFO - Checkpoints will be saved to /mnt/raid0/liuji/UniAD/projects/work_dirs/stage1_track_map/base_track_map by HardDiskBackend.
2025-04-22 08:34:08,223 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,224 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,252 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,494 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,511 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,538 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,545 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,554 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,555 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,563 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,585 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,598 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,602 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,602 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,615 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,625 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,629 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,646 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,667 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,717 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,720 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,738 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,739 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,750 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,750 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,763 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,766 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,777 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,778 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,792 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,804 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,805 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,805 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,813 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,830 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,857 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,872 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,886 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,901 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,905 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,926 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,935 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:08,943 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:08,957 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:08,959 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:08,994 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,016 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,023 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,039 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,077 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,081 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,093 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,097 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,101 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,110 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,115 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,133 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,139 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,154 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,165 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,172 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,175 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,179 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,190 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,201 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,227 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,230 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,240 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,255 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,271 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,292 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,312 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,323 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,341 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,342 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,380 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,399 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,401 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,408 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,419 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,424 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,455 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,461 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,471 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,488 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,492 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,508 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,508 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,509 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,525 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,534 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,545 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,560 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,570 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,586 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,820 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,835 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,837 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,851 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,858 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,871 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,874 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,880 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,891 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,896 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,897 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,899 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,911 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,913 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,917 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,926 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,933 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,958 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,962 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:09,971 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:09,987 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:09,989 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:09,992 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,010 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,029 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,052 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,057 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:10,076 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:10,118 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,131 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,162 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,181 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:10,236 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,243 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,253 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,259 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,290 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,295 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,305 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,314 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:10,317 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:10,322 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,361 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,385 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:10,449 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,467 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,509 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,536 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:10,564 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,572 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,585 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,590 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,631 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,633 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,660 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:10,661 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:10,722 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,741 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,776 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,781 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,795 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,806 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,806 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:10,817 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,823 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,834 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,836 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,845 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,860 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:10,862 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,869 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,874 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,879 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,892 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:10,895 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,898 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:10,904 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,923 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,924 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,929 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:10,934 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,941 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,941 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,959 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:10,965 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,966 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:10,977 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,977 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:10,977 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,981 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:10,999 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:11,000 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:11,004 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:11,018 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:11,021 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:11,042 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:11,294 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:11,300 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:11,311 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:11,317 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:11,352 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:11,354 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:11,359 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:11,373 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:11,380 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:11,389 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:11,413 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:11,420 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:11,434 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:11,449 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:11,477 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:11,506 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:11,530 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:11,545 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:11,578 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:11,599 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:11,619 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:11,638 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:11,676 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:11,695 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:11,698 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:11,710 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:11,744 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:11,766 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:11,897 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:11,913 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:11,915 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:11,930 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:11,956 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:11,970 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:11,984 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:11,984 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:11,998 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:12,002 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,043 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,067 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:12,111 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,124 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,129 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,141 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,168 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,172 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,179 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,189 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,191 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:12,199 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,199 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,202 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:12,216 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,216 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,226 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,250 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:12,252 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,253 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,275 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:12,277 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:12,281 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,297 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,328 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,332 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,341 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,351 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,355 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:12,366 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,366 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,383 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:12,398 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,420 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:12,473 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,491 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,498 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,517 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,530 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,557 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:12,558 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,582 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:12,787 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,807 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,850 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,879 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:12,896 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:12,899 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,911 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,917 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:12,918 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,923 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:12,926 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,942 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:12,947 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,948 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,955 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,956 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:12,958 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,961 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,962 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,963 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,969 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,970 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:12,976 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:12,984 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:12,992 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,993 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:12,999 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:13,000 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:13,011 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:13,014 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:13,017 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:13,019 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:13,039 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:13,064 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:13,082 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:13,098 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:13,119 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:13,137 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:13,140 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:13,163 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:13,251 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:13,267 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:13,302 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:13,326 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:13,394 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:13,412 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:13,451 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:13,457 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:13,475 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:13,478 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:13,481 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:13,497 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:13,512 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:13,513 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:13,530 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:13,534 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:13,535 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:13,557 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:13,568 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:13,594 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:13,734 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:13,750 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:13,788 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:13,801 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:13,810 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:13,818 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:13,854 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:13,879 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:13,936 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:13,963 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:14,021 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:14,036 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:14,067 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:14,089 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:14,227 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:14,255 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:14,287 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:14,303 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:14,338 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:14,339 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:14,357 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:14,361 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:14,363 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:14,377 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:14,394 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:14,414 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:14,419 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:14,428 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:14,437 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:14,441 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:14,444 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:14,455 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:14,478 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:14,489 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:14,502 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:14,509 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:14,633 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:14,667 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:14,676 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:14,691 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:14,726 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:14,749 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:14,770 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:14,798 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:14,862 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:14,878 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:14,911 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:14,933 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:14,975 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:14,991 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:14,997 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:15,012 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:15,027 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:15,048 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:15,050 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:15,072 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:15,275 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:15,309 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:15,382 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:15,398 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:15,416 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:15,422 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:15,526 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:15,542 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:15,577 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:15,598 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:15,627 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:15,654 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:15,658 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:15,674 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:15,701 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:15,706 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:15,716 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:15,728 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:15,748 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:15,770 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:15,818 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:15,834 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:15,867 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:15,890 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:15,982 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:15,998 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:16,032 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:16,054 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:16,147 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:16,168 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:16,293 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:16,309 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:16,321 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:16,338 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:16,462 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:16,495 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:16,510 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:16,538 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:16,734 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:16,750 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:16,785 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:16,807 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:16,843 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:16,864 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:16,933 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:16,938 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:16,949 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:16,955 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:16,982 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:16,986 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:16,987 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:16,999 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:17,003 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:17,006 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:17,023 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:17,039 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:17,194 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:17,201 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:17,209 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:17,228 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:17,245 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:17,270 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:17,357 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:17,367 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:17,379 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:17,387 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:17,706 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:17,730 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:17,877 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:17,889 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:17,915 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:17,932 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:18,018 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:18,034 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:18,062 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:18,070 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:18,092 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:18,093 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:18,165 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:18,173 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:18,188 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:18,191 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:18,219 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:18,221 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:18,234 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:18,242 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:18,267 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:18,288 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:18,418 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:18,432 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:18,457 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:18,473 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:18,582 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:18,604 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:18,607 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:18,619 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:18,652 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:18,667 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:18,676 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:18,694 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:18,913 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:18,929 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:18,933 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:18,959 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:18,963 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:18,984 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:19,018 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:19,044 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:19,190 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:19,206 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:19,239 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:19,261 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:19,379 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:19,390 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:19,416 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:19,432 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:19,460 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:19,476 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:19,510 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:19,531 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:19,846 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:19,857 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:19,874 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:19,882 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:19,898 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:19,901 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:20,027 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:20,040 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:20,076 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:20,093 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:20,104 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:20,108 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:20,118 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:20,132 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:20,159 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:20,189 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:20,197 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:20,226 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:20,397 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:20,411 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:20,442 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:20,455 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:20,464 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:20,469 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:20,501 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:20,521 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:20,589 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:20,605 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:20,639 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:20,640 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:20,656 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:20,661 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:20,691 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:20,713 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:21,003 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:21,014 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:21,043 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:21,060 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:21,239 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:21,251 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:21,276 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:21,292 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:21,372 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:21,388 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:21,422 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:21,444 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:21,542 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:21,557 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:21,590 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:21,612 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:21,614 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:21,629 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:21,663 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:21,685 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:21,711 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:21,721 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:21,744 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:21,758 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:21,789 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:21,803 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:21,804 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:21,818 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:21,837 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:21,850 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:21,859 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:21,871 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:22,129 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:22,142 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:22,169 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:22,185 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:22,336 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:22,348 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:22,375 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:22,392 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:22,463 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:22,479 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:22,513 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:22,534 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:22,674 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:22,685 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:22,711 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:22,728 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:22,733 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:22,748 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:22,780 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:22,794 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:22,802 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:22,809 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:22,841 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:22,862 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:22,899 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:22,914 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:22,948 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:22,961 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:22,970 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:22,976 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:23,011 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:23,033 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:23,308 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:23,321 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:23,349 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:23,366 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:23,435 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:23,447 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:23,475 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:23,492 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:23,587 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:23,603 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:23,637 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:23,659 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:23,946 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:23,961 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:23,993 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:24,015 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:24,031 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:24,048 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:24,083 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:24,106 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:24,206 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:24,221 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:24,254 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:24,274 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:24,586 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:24,597 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:24,624 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:24,641 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:25,199 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:25,215 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:25,217 - shapely.geos - INFO - Self-intersection at or near point 471.98647224679792 1711.8705119768197
2025-04-22 08:34:25,231 - shapely.geos - INFO - Self-intersection at or near point 1070.4578478045039 1612.8333503895974
2025-04-22 08:34:25,248 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:25,261 - shapely.geos - INFO - Self-intersection at or near point 1147.8004993016441 1404.4814343572746
2025-04-22 08:34:25,268 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:25,282 - shapely.geos - INFO - Self-intersection at or near point 1942.7006727221749 863.01771240972209
2025-04-22 08:34:26,742 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:26,763 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:27,406 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:27,425 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:28,086 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:28,106 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:28,744 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:28,763 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
2025-04-22 08:34:29,423 - shapely.geos - INFO - Self-intersection at or near point 134.36374450633136 1091.2903070826083
2025-04-22 08:34:29,443 - shapely.geos - INFO - Self-intersection at or near point 165.02390945024581 1095.7013529720464
Traceback (most recent call last):
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 256, in <module>
    main()
  File "/mnt/raid0/liuji/UniAD/./tools/train.py", line 245, in main
    custom_train_model(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/train.py", line 21, in custom_train_model
    custom_train_detector(
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/apis/mmdet_train.py", line 194, in custom_train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/epoch_based_runner.py", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/mmopenlab/mmcv/mmcv/parallel/data_parallel.py", line 77, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File "/mmopenlab/mmdetection/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 81, in forward
    return self.forward_train(**kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py", line 178, in forward_train
    losses_seg, outs_seg = self.seg_head.forward_train(bev_embed, img_metas,
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 119, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/panseg_head.py", line 1087, in forward_train
    pred_seg_dict = self(bev_feat)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 208, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/panseg_head.py", line 226, in forward
    enc_outputs_class, enc_outputs_coord = self.transformer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/runner/fp16_utils.py", line 208, in new_func
    return old_func(*args, **kwargs)
  File "/mnt/raid0/liuji/UniAD/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_deformable_transformer.py", line 318, in forward
    memory = self.encoder(query=feat_flatten,
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmdetection/mmdet/models/utils/transformer.py", line 479, in forward
    x = super(DetrTransformerEncoder, self).forward(*args, **kwargs)
  File "/mmopenlab/mmcv/mmcv/cnn/bricks/transformer.py", line 934, in forward
    query = layer(
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/cnn/bricks/transformer.py", line 855, in forward
    query = self.ffns[ffn_index](
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmopenlab/mmcv/mmcv/utils/misc.py", line 340, in new_func
    output = old_func(*args, **kwargs)
  File "/mmopenlab/mmcv/mmcv/cnn/bricks/transformer.py", line 623, in forward
    out = self.layers(x)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 80.00 MiB (GPU 0; 191.98 GiB total capacity; 43.14 GiB already allocated; 0 bytes free; 43.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 57728 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 57730 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 57731 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 57729) of binary: /opt/conda/bin/python
Traceback (most recent call last):
  File "/opt/conda/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py", line 766, in <module>
    main()
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-22_08:35:18
  host      : hjbog-srdc-20.amd.com
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 57729)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
